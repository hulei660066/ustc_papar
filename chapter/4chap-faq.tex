
\chapter{用户兴趣探索}
\label{chap:interestExplore}
电子商务产品的设计往往是数据驱动的，即许多产品方面的决策都是把用户行为量化后得出的。但就商品而言，那些热门主题往往只代表了用户一小部分的个性化需求，只有通过对用户行为的充分分析，才能更好的挖掘出用户的兴趣，最终提升商品的销售量。

为了更好的探索用户兴趣，手机主题推荐系统充分利用了用户画像和商品特征表。用户画像包括基本信息和兴趣特征向量,商品特征向量表包括分类、标签、适用人群等，给定某用户行为，用户兴趣探索过程分为如下几个步骤：首先，利用用户历史行为(评论,停留时长,评分,点赞,购买等)建模量化用户满意度,然后，利用用户兴趣特征向量与商品特征向量得出相关分数，如果商品与用户的相关分数很低,但有很高的用户满意度,说明是一次成功的用户兴趣探索,更新用户画像。如果是热门商品,大量的用户都会点击,但商品与用户不是很相关,则认为其探索效果是有限的,反之如果是小众商品,考虑到长尾效应,则可以认为其是更成功的兴趣探索。这里涉及到的关键概念包括用户满意度的量化、小众标签的定向挖掘、用户兴趣的动态化。

本章内容首先介绍海量用户行为数据的存储方式。用户行为数据拥有区别于传统数据库数据的特点有，用户行为数据量巨大，常面临TB甚至PB级的数据；含有较多的噪音；多维聚合式查询。针对这些特征，用户行为数据采用Hbase数据集群存储和hadoop集群计算。然后，介绍用户兴趣探索的算法模型；然后，介绍如何通过用户行为的分析量化用户满意度。最后，介绍小众兴趣标签的挖掘。

\section{用户行为数据的存储}
手机主题用户行为数据的特点包括：用户基数庞大。手机主题注册用户达千万级，活跃用户达百万级；用户规模增长快。月新注册用户达10万数量级。每个用户的行为数量较小。即使是活跃用户，每天最多也只能产生上百条行为记录，每年不超过十万条；用户行为的计算较为复杂。计算用户的两次登录间隔天数、反复购买的商品、累积在线时间，这些都是针对用户行为的计算，通常具有一定的复杂性；用户行为数据格式不规整，字段丢失率较高。根据用户行为数据的这些特点，我们采用基于Hadop分布式的架构。Hadop又如下几个优点：
\begin{itemize}
\item 高可靠性。Hadoop按位存储和处理数据的能力使其具有高可靠性。
\item 高扩展性。Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以根据用户增长规模方便地扩展到数以千计的节点中。
\item 高容错性。Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
\item 高效性。Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。
\item 低成本。hadoop是开源的，项目的软件成本因此会大大降低。
\end{itemize}
Hadoop的框架最核心的设计是HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。接下来首先介绍HDFS的体系架构，然后介绍MapReduce。
  \subsection{HDFS的体系架构}
  HDFS采用主从(Master/Slave)结构模型，一个HDFS集群是由一个NameNode和若干个DataNode组成的(在最新的Hadoop2.2版本已经实现多个NameNode的配置)。NameNode作为主服务器，管理文件系统命名空间和客户端对文件的访问操作。DataNode管理存储的数据。HDFS支持文件形式的数据。从内部来看，文件被分成若干个数据块，这若干个数据块存放在一组DataNode上。NameNode执行文件系统的命名空间，如打开、关闭、重命名文件或目录等，也负责数据块到具体DataNode的映射。DataNode负责处理文件系统客户端的文件读写，并在NameNode的统一调度下进行数据库的创建、删除和复制工作。NameNode是所有HDFS元数据的管理者，用户数据永远不会经过NameNode，HDFS体系结构图如\autoref{pic:hl_hadoop}所示。
  \begin{figure}
  \centering
    \framebox{\includegraphics[scale=0.4]{figures/hl_hadoop}}
    \figcaption{HDFS体系结构}
    \label{pic:hl_hadoop}
  \end{figure}

  其中，NameNode、DataNode、Client。NameNode是管理者，DataNode是文件存储者、Client是需要获取分布式文件系统的应用程序。HDFS作为分布式文件系统在数据管理方面设计了多重容错冗余：一个Block会有三份备份，一份在NameNode指定的DateNode上，一份放在与指定的DataNode不在同一台机器的DataNode上，一根在于指定的DataNode在同一Rack上的DataNode上。备份的目的是为了数据安全，采用这种方式是为了考虑到同一Rank失败的情况，以及不同数据拷贝带来的性能的问题。
  \begin{itemize}
  \item 文件写入：首先，Client向NameNode发起文件写入的请求；然后，NameNode根据文件大小和文件块配置情况，返回给Client它管理的DataNode的信息；最后，Client将文件划分为多个block，根据DataNode的地址，按顺序将block写入DataNode块中。
  \item 文件读取：首先，Client向NameNode发起读取文件的请求；然后，NameNode返回文件存储的DataNode信息；最后，Client读取文件信息。
  \end{itemize}

  \subsection{MapReduce体系架构}
  MR框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点上的TaskTracker共同组成。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的不同的从节点上。主节点监视它们的执行情况，并重新执行之前失败的任务。从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接受到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。JobTracker可以运行于集群中的任意一台计算机上。TaskTracker负责执行任务，它必须运行在DataNode上，DataNode既是数据存储节点，也是计算节点。JobTracker将map任务和reduce任务分发给空闲的TaskTracker，这些任务并行运行，并监控任务运行的情况。如果JobTracker出了故障，JobTracker会把任务转交给另一个空闲的TaskTracker重新运行。

  HDFS和MR共同组成Hadoop分布式系统体系结构的核心。HDFS在集群上实现了分布式文件系统，MR在集群上实现了分布式计算和任务处理。HDFS在MR任务处理过程中提供了文件操作和存储等支持，MR在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成分布式集群的主要任务。Hadoop上的并行应用程序开发是基于MR编程框架。MR编程模型原理：利用一个输入的key-value对集合来产生一个输出的key-value对集合。MR库通过Map和Reduce两个函数来实现这个框架。用户自定义的map函数接受一个输入的key-value对，然后产生一个中间的key-value对的集合。MR把所有具有相同的key值的value结合在一起，然后传递个reduce函数。Reduce函数接受key和相关的value结合，reduce函数合并这些value值，形成一个较小的value集合。通常我们通过一个迭代器把中间的value值提供给reduce函数，这样就可以处理无法全部放在内存中的大量的value值集合了。

  MapReduce体系中数据流动过程：首先，大数据集被分成众多小的数据集块，若干个数据集被分在集群中的一个节点进行处理并产生中间结果。然后，单节点上的任务，map函数一行行读取数据获得数据的(k1,v1)，数据进入缓存，通过map函数执行map(基于key-value)排序执行后输入(k2,v2)，有时候在map之后reduce之前有一个数据合并(Combine)操作，即将中间有相同的key的对合并，Combine能减少中间结果key-value对的数目，从而降低网络流量。最后，不同机器上的(k2,v2)通过merge排序的过程，reduce合并得到，(k3,v3)，输出到HDFS文件中。数据流示意图如\autoref{pic:hl_MapReduce}所示。值得一提的是，Map任务的中间结果在做完Combine和Partition后，以文件的形式存于本地磁盘上。中间结果文件的位置会通知主控JobTracker，JobTracker再通知reduce任务到哪一个DataNode上去取中间结果。所有的map任务产生的中间结果均按其key值按hash函数划分成R份，R个reduce任务各自负责一段key区间。每个reduce需要向许多个map任务节点取的落在其负责的key区间内的中间结果，然后执行reduce函数，最后形成一个最终结果。有R个reduce任务，就会有R个最终结果，很多情况下这R个最终结果并不需要合并成一个最终结果，因为这R个最终结果可以作为另一个计算任务的输入，开始另一个并行计算任务。这就形成了多个输出数据片段(HDFS副本)。
  \begin{figure}
  \centering
    \framebox{\includegraphics[scale=0.5]{figures/hl_MapReduce}}
    \figcaption{MapReduce数据流}
    \label{pic:hl_MapReduce}
  \end{figure}

  \subsection{Hbase数据管理}
  Hbase作为Hadoop数据仓库。与传统的mysql、oracle还是有很大的差别。NoSql数据库与传统关系型数据的区别有如下几个方面：
  \begin{itemize}
  \item Hbase适合大量插入同时又有读的情况。输入一个Key获取一个value或输入一些key获得一些value。
  \item Hbase的瓶颈是硬盘传输速度。Hbase的操作包括往数据里面insert数据，update的实际上也是insert，只是插入一个新的时间戳的一行。Delete数据也是insert，只是insert一行带有delete标记的一行。Hbase的所有操作都是追加插入操作。Hbase是一种日志集数据库。存储方式像是日志文件一样，是批量大量的往硬盘中写，通常都是以文件形式的读写。所以读写速度就取决于硬盘与机器之间的传输有多快。而Oracle的瓶颈是硬盘寻道时间。其经常的操作时随机读写。要update一个数据，先要在硬盘中找到这个block，然后将其读入内存，在内存中的缓存中修改，过段时间再回写回去。硬盘的寻道时间主要由转速来决定的，所以形成了寻道时间瓶颈。
  \item Hbase中数据可以保存许多不同时间戳的版本。数据按时间排序，因此Hbase特别适合寻找按照时间排序寻找Top n的场景。找出某个人最近浏览的主题，最近购买的N款主题包，N种行为等等，因此Hbase在互联网应用非常多。
  \item Hbase的局限。只能做很简单的Key-value查询。它适合有高速插入，同时又有大量读的操作场景。而这种场景又很极端，并不是每一个公司都有这种需求。在一些公司，就是普通的OLTP(联机事务处理)随机读写。在这种情况下，Oracle的可靠性，系统的负责程度又比Hbase低一些。而且Hbase局限还在于它只有主键索引，因此在建模的时候就遇到了问题。
  \item Oracle是行式数据库，而Hbase是列式数据库。列式数据库的优势在于数据分析这种场景。数据分析与传统的OLTP的区别。数据分析，经常是以某个列作为查询条件，返回的结果也经常是某一些列，不是全部的列。在这种情况下，行式数据库反应的性能就很低效。
  \end{itemize}

  \subsection{Hive数据管理}
  Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转换、加载，是一种可以存储、查询和分析存储在Hadoop中的大规模数据机制。可以把Hadoop下结构化数据文件映射为一张成Hive中的表，并提供类sql查询功能，除了不支持更新、索引和事务，sql其它功能都支持。可以将sql语句转换为MapReduce任务进行运行，作为sql到MapReduce的映射器。提供shell、JDBC/ODBC、Thrift等接口。优点是成本低可以通过类sql语句快速实现简单的MapReduce统计。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三个方面介绍。
  \begin{itemize}
  \item 元数据存储。Hive将元数据存储在RDBMS中，有三种方式可以连接到数据库：1）内嵌模式：元数据保持在内嵌数据库的Derby，一般用于单元测试，只允许一个会话连接；2）多用户模式：在本地安装Mysql，把元数据放到Mysql内；3）远程模式：元数据放置在远程的Mysql数据库。
  \item 数据存储。首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用于可以非常自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，这就可以解析数据了。其次，Hive中所有的数据都存储在HDFS中，Hive中包含4中数据模型：Tabel、ExternalTable、Partition、Bucket。Table类似与传统数据库中的Table，每一个Table在Hive中都有一个相应的目录来存储数据。例如：一个表zz，它在HDFS中的路径为：/wh/zz，其中wh是在hive-site.xml中由用户设定的数据仓库的目录，所有的Table数据(不含External Table)都保存在这个目录中。Partition类似于传统数据库中划分列的索引。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition数据都存储在对应的目录中。例如：zz表中包含ds和city两个Partition，则对应于ds=20140214，city=beijing的HDFS子目录为：/wh/zz/ds=20140214/city=Beijing;Buckets对指定列计算的hash，根据hash值切分数据，目的是为了便于并行，每一个Buckets对应一个文件。将user列分数至32个Bucket上，首先对user列的值计算hash，比如，对应hash=0的HDFS目录为：/wh/zz/ds=20140214/city=Beijing/part-00000;对应hash=20的，目录为：/wh/zz/ds=20140214/city=Beijing/part-00020。ExternalTable指向已存在HDFS中的数据，可创建Partition。和Table在元数据组织结构相同，在实际存储上有较大差异。Table创建和数据加载过程，可以用统一语句实现，实际数据被转移到数据仓库目录中，之后对数据的访问将会直接在数据仓库的目录中完成。删除表时，表中的数据和元数据都会删除。ExternalTable只有一个过程，因为加载数据和创建表是同时完成。世界数据是存储在Location后面指定的HDFS路径中的，并不会移动到数据仓库中。
  \item 数据交换。用户接口包括客户端、Web界面和数据库接口，元数据通常存储在关系数据库如Mysql，Derby中。
  \end{itemize}
本小节主要介绍了Hadoop分布式计算平台最核心的分布式文件系统HDFS、MapReduce处理过程，以及数据仓库工具Hive和分布式数据库Hbase，基本涵盖了Hadoop分布式平台的所有技术核心。从体系架构到数据定义到数据存储再到数据处理，Hadoop分布式存储、计算平台为海量用户行为的分析和用户兴趣探索提供了可能。接下来的章节先介绍用户行为数据的分析，包括数据预处理和异常数据监测，然后介绍用户兴趣探索模块，包括算法模型、用户满意度量化、小众兴趣标签的挖掘。

\section{用户行为数据的的预处理}
数据预处理是数据挖掘过程中一个重要步骤，当原始数据存在不一致、重复、含噪声、维度高等问题时，更需要进行数据的预处理，以提高数据挖掘对象的质量，最终达到提高数据挖掘所获模式知识质量的目的。
  \subsection{背景}
  随着手机主题市场交易规模的逐步增大，积累下来的业务数据和用户行为数据越来越多，这些用户数据往往是电子商务平台最宝贵的财富。目前在手机主题推荐系统中大量地应用到了机器学习和数据挖掘技术，例如个性化推荐、搜索排序、用户画像建模等等，为企业创造了巨大的价值。本节主要介绍在用户兴趣探索实践中的数据预处理与特征挖掘方法。数据预处理主要工作是：
  \begin{itemize}
  \item 从原始数据，如文本、图像或者应用数据中清洗出特征数据和标注数据
  \item 对清洗出的特征和标注数据进行处理，例如样本采样，样本调权，异常点去除，特征归一化处理等过程。最终生成的数据主要是供模型直接使用。
  \end{itemize}

  \subsection{特征提取}
  用户兴趣探索的任务包括：探索用户的兴趣广度、兴趣深度、兴趣变动趋势。依据这些信息，推荐系统就能知道在面对某一个用户时要推荐哪几类型商品，每类商品所占的比例，未来几天推荐内容会有哪些变化。在确定了目标之后，接下来需要确定使用哪些数据来达到目标。提取哪些特征数据可能与用户是否点击购买相关，一方面可以借鉴一些业务经验，另一方面可以采用一些特征选择、特征分析等方法。从业务经验来判断，可能影响用户是否点击下单的因素有：
  \begin{itemize}
  \item 用户历史行为。对于老用户，之前可能有过点击、购买等行为。
  \item 用户实时兴趣。
  \item 用户满意度。上面的特征都是比较好衡量的，用户满意度可能是更复杂的一个特征，具体体现在用户评分、评价、购买后使用频率、时长等。
  \item 是否热门，商品评价人数，购买数等。
  \end{itemize}
  在确定好要使用哪些数据之后，还需要对使用数据的可用性进行评估，包括数据的获取难度，数据的规模，数据的准确率，数据的覆盖率等。
  \begin{itemize}
  \item 用户历史行为。只有老用户才会有行为，新用户是没有的。
  \item 数据获取难度。获取用户id不难，但是获取用户年龄和性别较困难，因为用户注册或者购买时，这些并不是必填项，即使填了也不完全准确。如果一些特征需要通过其他预测模型交叉验证的话，就存在着模型精度的问题。
  \item 数据覆盖率。数据覆盖率也是一个重要的考量因素，例如地理位置特征，并不是所有用户的距离我们都能获取到，PC端的就没有地理位置，还有很多用户禁止使用它们的定位功能。
  \item 用户实时行为。如果用户刚打开app，还没有任何行为，同样面临着一个冷启动的问题。
  \item 数据的准确率。有时候用户购买一款主题，不一定是其真心喜欢，可能是因为遇到限时半价、购买返现等活动。
  \end{itemize}

  \subsection{特征获取方式}
  特征提取方式分为在线提取和离线提取。

  离线特征获取方案。离线可以使用海量的数据，借助于分布式文件存储平台，例如HDFS等，使用例如MapReduce，Spark等处理工具来处理海量的数据等。

  在线特征获取方案。在线特征比较注重获取数据的延时，由于是在线服务，需要在非常短的时间内获取到相应的数据，对查找性能要求非常高，可以将数据存储在索引、key-value存储等，也可以使用Kafka等处理工具。Kafka是一种分布式的，基于发布/订阅的消息系统。主要设计目标如下：1）以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能；2）高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输；3）同时支持离线数据处理和实时数据处理；4）分布式系统，易于向外扩展。所有的producer、broker和consumer都会有多个，均为分布式的。无需停机即可扩展机器。

  \subsection{用户行为数据预处理}
  根据不同业务数据的预处理方式也不同，一般来讲原始服务器日志数据脏数据的形成原因包括：缩写词不统一，数据输入错误，不同的惯用语，重复记录，丢失值，不同的计量单位，过时的编码等。相应的，数据预处理内容包括数据清理、数据异常值检测、数据集成、数据变换、数据归约、数据离散化。

  1）数据清理包括格式标准化、异常数据清除、错误纠正、重复数据的清除。对于手机主题用户数据来讲，引起空缺值的原因主要是用户设备异常造成的，有些时候是因为与其他已有数据不一致而被删除或数据的改变没有进行日志记载。根据数据空缺情况的不同有不同的处理方式：
  \begin{itemize}
  \item 忽略元组。当一个记录中有多个属性值空缺、特别是关键信息丢失时,已不能反映真实情况,它的效果非常差。
  \item 去掉属性。缺失严重时,已无挖掘意义。
  \item 人工填写空缺值。但是工作量大且可行性低。
  \item 默认值。比如使用unknown或-∞。
  \item 使用属性的平均值填充空缺值。
  \item 预测最可能的值填充空缺值。使用贝叶斯公式或判定树这样的基于推断的方法。
  \end{itemize}

  2）数据异常值就是一个测量变量中的随机错误或偏差，对于手机主题用户数据来讲，引起不正确属性值的原因包括恶意hack行为、数据输入、传输错误等。根据面临的业务不同有不同的处理方式：
  \begin{itemize}
  \item 基于统计的异常值检测。首先建立一个数据统计模型，异常是那些模型不能完美拟合的对象，对于常用的回归模型，如\autoref{pic:hl_regression}所示，异常是相对远离预测值的对象。需要注意的是，离群点是一个对象，关于数据的概率分布模型，它具有低概率。这种情况的前提是必须知道数据集服从什么分布。
  \begin{figure}
  \centering
    \framebox{\includegraphics[scale=0.5]{figures/hl_regression}}
    \figcaption{回归异常值检测}
    \label{pic:hl_regression}
  \end{figure}
  \item 基于密度的异常值检测。离群点是在低密度区域中的对象，一个对象的离群点得分是该对象周围密度的逆。基于密度的离群点检测与基于邻近度的离群点检测密切相关，因为密度通常用邻近度定义。一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。需要小心的选择d，如果d太小，则许多正常点可能具有低密度，从而具有高离群点得分。如果d太大，则许多离群点可能具有与正常点类似的密度。
  \item 基于聚类的异常值检测。如\autoref{pic:hl_cluster}所示。一种利用聚类检测离群点的方法是丢弃远离其他簇的小簇。这个方法可以和其他任何聚类技术一起使用，但是需要最小簇大小和小簇与其他簇之间距离的阈值。这种方案对簇个数的选择高度敏感。使用这个方案很难将离群点得分附加到对象上。一种更系统的方法，首先聚类所有对象，然后评估对象属于簇的程度。基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类。还有一种更复杂的方法：取一组不能很好的拟合任何簇的特殊对象，这组对象代表潜在的离群点。随着聚类过程的进展，簇在变化。不再强属于任何簇的对象被添加到潜在的离群点集合；而当前在该集合中的对象被测试，如果其现在强属于一个簇，就可以将其从潜在的离群点集合中移除。
  \begin{figure}
  \centering
    \framebox{\includegraphics[scale=0.5]{figures/hl_cluster}}
    \figcaption{聚类异常值检测}
    \label{pic:hl_cluster}
  \end{figure}
  \end{itemize}

  3）数据集成就是将多个数据源中的数据整合到一个一致的存储中，需要注意以下几个情况：
  \begin{itemize}
  \item 模式集成。整合不同数据源中的元数据时的实体识别问题，比如匹配俩个表中的用户ID，A.custId=B.customerNo。
  \item 检测/解决数值冲突。对现实世界中的同一实体,来自不同数据源的属性值可能有所不同，如同表示停留时长，A表单位是秒，B表单位为毫秒。
  \item 多表之间的数据冗余。同一属性在不同的数据库中会有不同的字段名，有些时候冗余可以被相关分析检测出来，计算公式如所示，其中$\overline{A}$和$\overline{B}$表示为字段A和B的平均值，$\sigma_A和\sigma_B$表示其的标准差。仔细将多个数据源中的数据集成起来,能够减少或避免结果数据中的冗余与不一致性,从而可以提高挖掘的速度和质量。
  \begin{equation}
    r_{A,B} = \frac{\sum (A-\overline{A})(B-\overline{B})}{(n-1)\sigma_A\sigma_B}
    \label{F-Measure}
  \end{equation}
  \end{itemize}

  4）数据变换包括数据的平滑变换、数据聚集和数据规范化。所谓规范化是指将数据按比例缩放,使之落入一个小的特定区间，有如下几种方式：
  \begin{itemize}
  \item 最小-最大规范化。如\autoref{equ:minMax}所示，原始数值范围为[min,max]，通过公式映射到新区间[newMin,newMax]，$v'$表示属性v的公式映射。
  \begin{equation}
    v' = \frac{v-min}{max-min}(newMax-newMin)+newMin
    \label{equ:minMax}
  \end{equation}
  \item z-score规范化。Z-score表示原始数据偏离均值的距离长短，而该距离度量的标准是标准方差，如果统计数据量足够多，Z-score数据分布可以满足，68\%的数据分布在“-1”与“1”之间，95\%的数据分布在“-2”与“2”之间，99\%的数据分布在“-3”与“3之间”。如\autoref{equ:zscore}所示，其中v是原始数据，v'为v的映射，mean是全部数据的均值，$\sigma$为标准方差。
  \begin{equation}
    v' = \frac{v-mean}{\sigma}
    \label{equ:zscore}
  \end{equation}
  \item 小数定标规范化，小数定标规范化通过移动数据A的小数点位置进行规范化，小数点的移动位置依赖数据A的最大值。如\autoref{equ:small}所示，其中j是使$Max(| v'|)<1$的最小整数。
  \begin{equation}
    v' = \frac{v}{10^j}
    \label{equ:small}
  \end{equation}
  \end{itemize} 

  5）数据归约是数据字段从源数据集中得到数据集的归约表示。数据仓库中往往存有海量数据,在其上进行复杂的数据分析与挖掘需要很长的时间，通过数据归约使得数据小得多,且可以产生相同的分析结果，需要注意的是用于数据归约的时间不应当超过或“抵消”在归约后的数据上挖掘节省的时间。数据归约策略包括：
  \begin{itemize}
  \item 数据立方体聚集。最底层的方体对应于基本方体，基本方体对应于感兴趣的实体。在数据立方体中存在着不同级别的汇总，每次较高层次的抽象将进一步减少结果数据。数据立方体提供了对预计算的汇总数据的快速访问，所有尽可能对于汇总数据的查询使用数据立方体。
  \item 维归约，通过删除不相干的属性或维减少数据量，维归约又属性子集选择和启发式俩种实现方式。属性子集选择是指找出最小属性集,使得数据类的概率分布尽可能的接近使用所有属性的原分布，同时减少出现在发现模式上的属性的数目,使得模式更易于理解。启发式的方法有：逐步向前选择；逐步向后删除；向前选择和向后删除相结合；判定归纳树；基于统计分析的归约如主成分分析、回归分析等。
  \end{itemize}

  6)数据离散化,即连续属性的范围划分为区间，减少给定连续属性值的个数，区间的标号可以代替实际的数据值。
  \begin{itemize}
  \item 概念分层。通过使用高层的概念替代底层的属性值，如用青年、中年、 老年代替年龄数据值。
  \item 分箱（binning）。分箱技术递归的用于结果划分,可以产生概念分层。
  \item 直方图分析（histogram）。直方图分析方法递归的应用于每一部分,可以 自动产生多级概念分层。
  \item 聚类分析。将数据划分成簇,每个簇形成同一个概念层上的一个节点,每个簇可再分成多个子簇,形成子节点。
  \item 基于熵的离散化。
  \item 自然划分。将数值区域划分为相对一致的、易于阅读的、看上去更直观或自然的区间，划分步骤：如果一个区间最高有效位上包含3,6,7或9个不同的值,就将该区间划分为3个等宽子区间；如果一个区间最高有效位上包含2,4,或8个不 同的值,就将该区间划分为4个等宽子区间；如果一个区间最高有效位上包含1,5,或10个 不同的值,就将该区间划分为5个等宽子区间；将该规则递归的应用于每个子区间,产生给定数值属性的概念分层；对于数据集中出现的最大值和最小值的极端分布,为了避免上述方法出现的结果扭曲,可以在顶层分段时,选用一个大部分的概率空间，如5\%-95\%。
  \end{itemize}

\section{用户兴趣探索的算法模型}
用户兴趣探索就是要通过大数据挖掘建立一个用户兴趣模型，模型中包含用户的一个或更多个兴趣。首先针对本章节涉及到的对象域做出限定解释。

  \subsection{基本概念概述}
  \begin{itemize}
  \item 实体域。当我们想基于用户行为分析来建立用户兴趣模型时，我们必须把用户行为和兴趣主题限定在一个实体域上。个性化推荐落实在具体的推荐中都是在某个实体域的推荐。对于手机主题应用市场来说，实体域包括所有的主题，背景图片，铃声，闹铃等。兴趣主题。
  \item 用户行为。浏览，点击，下载，试用，购买，评论等都可是用户行为。本文所指的用户行为都是指用户在某实体域上的行为。比如用户在手机主题产生的行为。
  \item 用户兴趣。用户的兴趣维度，同样是限定在某实体域的兴趣，通常以标签+权重的形式来表示。比如，对于手机主题，用户兴趣向量可以是「动漫，0.6」，「体育，0.1」，「情感，0.7」等分类标签。值得一提的是，用户兴趣只是从用户行为中抽象出来的兴趣维度，并无统一标准。而兴趣维度的粒度也不固定，如「体育」，「电影」等一级分类，而体育下有「篮球」，「足球」等二级分类，篮球下有「NBA」，「CBA」，「火箭队」等三级分类。我们选取什么粒度的兴趣空间取决于具体用户兴趣模型。
  \item 兴趣空间。在同一层次上兴趣维度的集合，比如手机主题中，可以用「新上架」，「热门」，「特价」，「免费」来构成一个兴趣空间，也可以用「二次元」，「萝莉」，「魔幻」，「纯真」，「召唤兽」·····「法术」构成一个动漫兴趣空间。
  \end{itemize}
用户满意度的量化；小众兴趣标签的挖掘；动态兴趣;然后提出一种分布式的大规模并行聚类算法,并将该算法应用到该三部图模型中；基于浏览内容的兴趣度模型［７－８］、基于用户浏览行为的兴趣度模型［９］和动态变化的用户兴趣模型
